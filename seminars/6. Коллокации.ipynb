{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57e8fe2",
   "metadata": {},
   "source": [
    "Коллокации - это устойчивые выражения, состоящие из двух и более слов. Устойчивые - значит, что они часто используются вместе. Также часто значения коллокации не могут быть выведены лишь из значений, входящих в них слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95cefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5662f825",
   "metadata": {},
   "source": [
    "Зададим несколько функций, которые будут обрабатывать наши тексты для большей объективности и чистоты: очистим от стоп-слов и лемматизируем. \n",
    "Функция **normalize** лемматизирует наш текст иоставит от них только кириллические слова и числа, приведет текст к нижнему регистру, а затем выкинет все слова короче 3 букв.  \n",
    "Также нам нужен **sent_tokenize** из nltk.\n",
    "**ngrammer** cобирает нграммы (заданной величины) и их частоты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02853050",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('russian') + [\"это\", \"весь\"]) \n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [morph.parse(token)[0].normal_form for token in word_tokenize(text.lower()) if len(token) > 2 and token not in stops]  \n",
    "    return normalized_text\n",
    "\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    tokens_cl = [token for token in word_tokenize(tokens.lower()) if token not in stops]\n",
    "    for i in range(0, len(tokens_cl)-n+1):\n",
    "        ngrams.append(tuple(tokens_cl[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef8471-21fb-4a9c-81ab-63b578fc33d6",
   "metadata": {},
   "source": [
    "Результат работы функции **normalize**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2eaab8-c0ee-4db6-971f-5766d2240207",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize('Я люблю кошек и собак. Я люблю фикусы и кактусы')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4509813d-ef8b-4b7d-8366-3aaf43dfdd40",
   "metadata": {},
   "source": [
    "Результат **sent_tokenize**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b0f5b-57e6-4432-87ef-9fcfec29d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sent_tokenize('Я люблю кошек и собак. Я люблю фикусы и кактусы'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baf4e46-19d9-456b-be21-4b42a8017512",
   "metadata": {},
   "source": [
    "Результат **ngrammer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf1ede-d140-47b5-9a14-c03c1b6d25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammer('Я люблю кошек и собак. Я люблю фикусы и кактусы')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16315cbd-ae6c-4e1b-9b92-67ee19b888fa",
   "metadata": {},
   "source": [
    "Откроем письма Достоевского, разобъем на предложения и предобработаем каждое предложение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee931759",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'DostoyevskiEpistola.txt'\n",
    "with open(path, encoding='utf-8') as txt:\n",
    "    corpus = txt.read()\n",
    "    corpus = re.sub(r'\\n', ' ', corpus)\n",
    "    corpus = re.sub(r'[a-zA-Z…]',  '', corpus)\n",
    "    corpus_sent = sent_tokenize(corpus) #разбиение на предложения\n",
    "    corpus_sent_clean = [] #cюда мы сохраняем чистые предложения\n",
    "    for sent in corpus_sent: #пройдемся по каждому предложению\n",
    "        corpus_sent_clean.append(' '.join(normalize(sent))) #предобработаем\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4d02d-a09c-457a-b4fc-af2d30c799c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sent_clean[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01517d82-37c9-474a-8bd7-0d154b03700e",
   "metadata": {},
   "source": [
    "Получим с помощью счётчика все биграммы с их частотами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ea55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()\n",
    "\n",
    "for sent in corpus_sent_clean:\n",
    "    word_counter.update(ngrammer(sent, n=2))\n",
    "word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f936e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f8eeb-d46a-4bf9-9d09-f9a32a6047bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e34cb",
   "metadata": {},
   "source": [
    "В списке есть биграммы, которые попали в список из-за того, что одно слово очень частотное и вообще встречается много в каких контекстах. Нас скорее интересуют случаи, когда слова в большинстве случаев встречаются вместе. Для этого мы можем придумать какие-нибудь формулы, учитывающие частоты слов по отдельности и общую частоту.  \n",
    "\n",
    "Самый простой способ - взять количество упоминаний биграма и поделить на сумму количеств упоминаний слов по отдельности.  \n",
    "\n",
    "Такая формула называется PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_simple(word_count_a, word_count_b, bigram_count, *args): #*args позволяет подать в функцию любое количество аргументов\n",
    "    try:\n",
    "        score = bigram_count / (word_count_a + word_count_b)\n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return score\n",
    "\n",
    "#Сделаем функцию, которая будет делать счетчики для слов и биграммов.\n",
    "def collect_stats(corpus, stops):\n",
    "    ## соберем статистики для отдельных слов\n",
    "    ## и биграммов\n",
    "    \n",
    "    unigrams = Counter()\n",
    "    bigrams = Counter()\n",
    "    \n",
    "    for sent in corpus:\n",
    "        unigrams.update(word_tokenize(sent))\n",
    "        bigrams.update(ngrammer(sent))\n",
    "    \n",
    "    return unigrams, bigrams\n",
    "\n",
    "#И функцию, которая пройдет по всем биграммам и вычислит для них нашу метрику.\n",
    "def score_bigrams(unigrams, bigrams, scorer, threshold=-100000, min_count=1):\n",
    "    ## посчитаем метрику для каждого нграмма\n",
    "    bigram2score = Counter()\n",
    "    len_vocab = len(unigrams)\n",
    "    for bigram in bigrams:\n",
    "        score = scorer(unigrams[bigram[0]], unigrams[bigram[1]], \n",
    "                       bigrams[bigram], len_vocab, min_count)\n",
    "        \n",
    "        ## если метрика выше порога, добавляем в словарик\n",
    "        if score > threshold:\n",
    "            bigram2score[bigram] = score\n",
    "    \n",
    "    return bigram2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams, bigrams = collect_stats(corpus_sent_clean, stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(unigrams)\n",
    "#print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score = score_bigrams(unigrams, bigrams, scorer_simple)\n",
    "bigram2score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef284b7",
   "metadata": {},
   "source": [
    "Проблема с таким подходом в том, что на самом верху окажутся слова, которые встречают по одному разу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7fee35",
   "metadata": {},
   "source": [
    "Поэтому можно немного переделать оценивающую функцию, добавив минимальное число вхождений для биграмма."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b2f09",
   "metadata": {},
   "source": [
    "По этой ссылке можно прочитать про другие метрики.  \n",
    " \n",
    "http://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S1405-55462016000300327#t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccceca12",
   "metadata": {},
   "source": [
    "**Все готовое**  \n",
    "\n",
    "Писать все это самому не обязательно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed35fac",
   "metadata": {},
   "source": [
    "Самый удобный способ - использовать nltk. Тут больше метрик, но преборазователь слов в нграммы нужно написать самому."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a455be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fca2f8-6187-4fcb-8ef7-5a046065b21e",
   "metadata": {},
   "source": [
    "Список названий коллокационных мер, доступных в nltk находится здесь (таблица Methods): https://tedboy.github.io/nlps/generated/generated/nltk.BigramAssocMeasures.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528d4f8-660c-421e-b4d6-81ab9221da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ' '.join(corpus_sent_clean)\n",
    "corpus[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e9da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4267d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_big = BigramCollocationFinder.from_words(word_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0583a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_trig = TrigramCollocationFinder.from_words(word_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76293d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb0a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_big.nbest(bigram_measures.raw_freq, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5670be",
   "metadata": {},
   "source": [
    "Получим значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = finder_big.score_ngrams(bigram_measures.chi_sq)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec564d",
   "metadata": {},
   "source": [
    "Отсортируем в обратном порядке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73579925",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = sorted(scores, key=lambda score: score[1], reverse=False)\n",
    "sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c86f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_trig.nbest(trigram_measures.pmi, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5d242-cf3c-4151-992a-c6d44aefbf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = finder_trig.score_ngrams(trigram_measures.pmi)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f460a",
   "metadata": {},
   "source": [
    "https://www.nltk.org/howto/collocations.html - дополнительные опции можно увидеть здесь."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f406c-3889-4918-8959-b0635427d537",
   "metadata": {},
   "source": [
    "Можно написать взвешенную меру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d248fd0-1fae-4ee8-a23e-39229757b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = corpus[:100000]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df936e-937a-4e29-a28e-c5990249e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_big = BigramCollocationFinder.from_words(word_tokenize(text))\n",
    "scores_lhr = finder_big.score_ngrams(bigram_measures.likelihood_ratio)\n",
    "scores_raw = finder_big.score_ngrams(bigram_measures.student_t)\n",
    "scores_pmi = finder_big.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c6d55-81f3-41f4-a9d0-3f8d39cd4649",
   "metadata": {},
   "source": [
    "Подпишем ранг у каждой из мер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d1018-3d5f-46a9-8ba6-64d974c5d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_lhr = []\n",
    "counter = 1\n",
    "for colloc in scores_lhr:\n",
    "    rank_lhr.append((colloc[0], counter))\n",
    "    counter += 1\n",
    "\n",
    "rank_lhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af617e-1229-4ce6-b0d3-125532313363",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_raw = []\n",
    "counter = 1\n",
    "for colloc in scores_raw:\n",
    "    rank_raw.append((colloc[0], counter))\n",
    "    counter += 1\n",
    "\n",
    "rank_pmi = []\n",
    "counter = 1\n",
    "for colloc in scores_pmi:\n",
    "    rank_pmi.append((colloc[0], counter))\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f51285-3ba9-43cd-80b6-a8bf384da142",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0a8b8-322f-4767-b729-6f0e1f3965aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361bdf1-cc93-4bda-9fef-ffb93b369b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ranks = [rank_lhr, rank_raw, rank_pmi]\n",
    "all_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07955a4f-4209-4944-ada5-9c4e0b1d4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_ranks = {}\n",
    "for bigs_ranks in all_ranks:\n",
    "   # print(bigs_ranks)\n",
    "    bigs, ranks = zip(*bigs_ranks) #звездочка разделяет кортежи\n",
    "    #print(bigs)\n",
    "    for index in range(len(ranks)):\n",
    "        #print(index)\n",
    "        if bigs[index] not in sum_of_ranks:\n",
    "            sum_of_ranks[bigs[index]] = 0\n",
    "        sum_of_ranks[bigs[index]] += ranks[index]\n",
    "\n",
    "sum_of_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22084734-dabb-4093-9b4c-25ebc72de25d",
   "metadata": {},
   "source": [
    "Можно сделать списком:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ff923-873b-4d0d-8c1f-4491e195f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = list(sum_of_ranks.items())\n",
    "sorted(sorted_list, key=lambda sorted_list: sorted_list[1], reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3de617-c04d-4bae-98a3-6584068e9a21",
   "metadata": {},
   "source": [
    "**Задание 1.** Выберите любой достаточно большой текст. Посчитайте коллокации (2-, 3-, 4- - любые) на нем с помощью любых трех мер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69edd6f-7c75-4d58-bd40-e4ee6d40db93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27e2f073-497b-4a83-a1eb-928bf515bb88",
   "metadata": {},
   "source": [
    "**Задание 2.** Теперь посчитайте сумму рангов. Какие коллокации взвешенно наиболее весомые в вашем тексте?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede340ac-4a9e-41b9-8b5d-5e6f96f747ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d157b90-664d-419b-9599-9ab761cd19bb",
   "metadata": {},
   "source": [
    "**NB!** Коллокации можно извлекать не только по стоящим непосредственно друг перед другом токенами. Можно также указывать окно, в котором будет проводиться поиск коллокации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6316b-19e9-4182-8316-934217a84e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_big_win = BigramCollocationFinder.from_words(word_tokenize(text), window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f9d5f-d8f8-4bf6-852c-b7680a4e5f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tscore_no_win = finder_big.score_ngrams(bigram_measures.student_t)\n",
    "scores_tscore_win = finder_big_win.score_ngrams(bigram_measures.student_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818ced1-4d62-4d65-bd9b-f60740a744cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_tscore_no_win[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc0bac-ee37-46ef-ac5f-cee372987a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_tscore_win[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d360a45-f4d4-49aa-9117-5ee88bef4643",
   "metadata": {},
   "source": [
    "Как видно, коллокации отличаются."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f2082a-3046-4e6e-a137-fc83bd6a36fb",
   "metadata": {},
   "source": [
    "**Задание 3**. Посчитайте метрики на разных окнах для текста из заданий 1-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ef37a-5f7c-43d0-9209-85ade91d00fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a6e27e2-3d5b-4cc3-94b1-30bafd77b321",
   "metadata": {},
   "source": [
    "Теперь учтём частеречную разметку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9775357d-86ea-4a11-88e3-3752b8acc8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ' '.join(corpus_sent_clean)\n",
    "text = corpus\n",
    "tagged_tuples = ['_'.join((str(morph.parse(token)[0].normal_form), str(morph.parse(token)[0].tag.POS)) for token in word_tokenize(text)]\n",
    "tagged_text = ' '.join(tagged_tuples)\n",
    "tagged_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a48a41-2121-4a4d-a3e7-ea8a5a53d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2504026-8176-449c-b170-21cd08c75097",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_big = BigramCollocationFinder.from_words(word_tokenize(tagged_text))\n",
    "scores_student_t = finder_big.score_ngrams(bigram_measures.student_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711c0d9-0867-4fad-b8c4-da9d2db6c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_student_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59558ed6-64cb-4583-9ec1-998815b5fce2",
   "metadata": {},
   "source": [
    "Теперь найдем все биграммы, в которых первое слово - существительное с помощью регулярки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d798257-211a-4273-a41d-c2b954d2a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13179ea0-e2c8-4c98-84ce-40048d43dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_with_nouns = []\n",
    "for bigram_measured in scores_student_t:\n",
    "    #print(bigram_measured)\n",
    "    #print(bigram_measured[0][0])\n",
    "    if re.match(r'\\w+_NOUN', bigram_measured[0][0]):\n",
    "        #print(bigram_measured[0][0])\n",
    "       # print(bigram_measured)\n",
    "        bigrams_with_nouns.append(bigram_measured)\n",
    "print(bigrams_with_nouns[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1d4b3-fea6-4614-a2a2-b2b7b6dffdac",
   "metadata": {},
   "source": [
    "**Задание 4.** Как переписать код выше, чтобы он искал биграммы, в которых второе слово - существительное, а первое любое?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828f92c-1319-471f-8987-c8fbc38ab6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d7eea62-2e63-4536-9372-c0c4609abb04",
   "metadata": {},
   "source": [
    "**Задание 5.** Как переписать код выше, чтобы он искал биграммы, в которых первое слово прилагательное, а второе слово - существительное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c8030-178f-4e13-8953-715b082af032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
