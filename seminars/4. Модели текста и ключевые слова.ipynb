{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992e06b5",
   "metadata": {},
   "source": [
    "Эта семинарская тетрадка дополняет слайды презентации, посвященные моделям **мешка слов** и **tf-idf**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b0ac82-8969-40b1-815b-dc8fbbecdf83",
   "metadata": {},
   "source": [
    "Для начала построим модель мешка слов на известных уже нам строфах из Пушкина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc43170-04db-4386-9e9d-b685c58e3240",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Мой дядя самых честных правил, Когда не в шутку занемог, Он уважать себя заставил И лучше выдумать не мог. Его пример другим наука; Но, боже мой, какая скука С больным сидеть и день и ночь, Не отходя ни шагу прочь! Какое низкое коварство Полуживого забавлять, Ему подушки поправлять, Печально подносить лекарство, Вздыхать и думать про себя: Когда же чёрт возьмёт тебя?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe8965-6b5f-4c55-897c-3420ed87933c",
   "metadata": {},
   "source": [
    "Текст, который мы подадим для создания модели текста, лучше предварительно обработать. Например, удалить стоп-слова и лемматизировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046aca10-2605-42d3-a145-eeaa24a3e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "stopwords_ru = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ad478-d2c8-45b4-a147-73cbaa8cc305",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub('[^а-яА-ЯёЁ -]', '', text.lower())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ac1ef-4d70-451b-910d-116b4544da2c",
   "metadata": {},
   "source": [
    "Лемматизируем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d245b2d-21a4-428a-98ff-143b6f543191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy3 import MorphAnalyzer \n",
    "morph = MorphAnalyzer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179c953-1710-4f57-b83b-c69823fa52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_text = [morph.parse(tok)[0].normal_form for tok in word_tokenize(text)]\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d990c-92d2-4658-ba0e-78f2e99d0865",
   "metadata": {},
   "source": [
    "Удалим стоп-слова (да, обычно их лучше удалять после лемматизации):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edbefd3-31e3-472a-9276-98a19d43df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_no_stop = [' '.join([token for token in lemmatized_text if token not in stopwords_ru])]\n",
    "print(text_no_stop) #мы склеили обратно и поместили в список, потому что векторизатор в sklearn принимает список строк"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8310b14-23f3-448c-9041-356b1dfecdb5",
   "metadata": {},
   "source": [
    "Модель мешка слов очень легко создается с помощью библиотеки **sklearn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f648c-a6b9-4aac-83c7-685c7d10e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём модель мешка слов\n",
    "from sklearn.feature_extraction.text import CountVectorizer #импорт функции, которая создаст вектора мешка слов\n",
    "vectorizer = CountVectorizer() #сохраним в переменную\n",
    "X = vectorizer.fit_transform(text_no_stop) #fit transform cоздает модель мешка слов\n",
    "print(X) #пока тут ничего полезного"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218c901-f64c-47dc-b53f-48fda08b9c63",
   "metadata": {},
   "source": [
    "Теперь получим список уникальных слов, который входит в мешок слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dbfa4c-c406-4875-ad7c-39368b0b5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7d974-0ed0-4d25-aad3-facddc4b6f66",
   "metadata": {},
   "source": [
    "Чтобы получить значения в матрице мешка слов, нужно сделать следующее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1952f-c0ed-4fdd-b1ff-0f23a6817689",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6c859-50a8-4934-8316-32ef96133441",
   "metadata": {},
   "source": [
    "И сделаем теперь вектор наглядным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3fe1e3-5088-4228-a289-301c164e424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bedf6-aa01-43a7-be1c-a5f0d713dbb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_vector = pd.DataFrame({'words': vectorizer.get_feature_names_out(),\n",
    "                            'vectors': X.toarray()[0]})\n",
    "text_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c450f2-37bf-4e5f-b045-dee9d12e726d",
   "metadata": {},
   "source": [
    "Здесь единый и короткий текст, где все леммы являются гапаксами. Давайте возьмем корпус чуть больше, при этом это будут разные документы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bee5b4-a819-48b2-aa72-77495516cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Aleksandr\\\\Downloads\\\\EugeneOnegin.txt', encoding='utf-8') as txt:\n",
    "    text = txt.read()\n",
    "    corpus = re.split(r'ГЛАВА \\w+\\b', text)\n",
    "    print(len(corpus))\n",
    "    clean_texts = []\n",
    "    for text in corpus:\n",
    "        text = re.sub(r'\\n', ' ', text)\n",
    "        text = re.sub('[^а-яА-ЯёЁ -]', '', text.lower())\n",
    "        lemmatized_text = [morph.parse(tok)[0].normal_form for tok in word_tokenize(text)]\n",
    "        text_no_stop = ' '.join([token for token in lemmatized_text if token not in stopwords_ru])\n",
    "        clean_texts.append(text_no_stop)\n",
    "print(clean_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f878f7-5920-4902-b88e-7a93137cd492",
   "metadata": {},
   "source": [
    "Теперь составим векторные модели мешка слов для каждой из глав:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a65b920-acfd-43d9-8c3b-1404ed7b542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(clean_texts) #fit transform cоздает модель мешка слов\n",
    "len(vectorizer.get_feature_names_out()) #количество уникальных лемм в нашей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7f7bc-f58a-4432-ad84-01a813cf18a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a8919-8b06-45dd-a576-7d86f1806109",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122b6f1-1227-4324-8b7c-1baa7e319afb",
   "metadata": {},
   "source": [
    "Зададим таблицу несколько иначе, потому что вектора текстов удобнее располагать другим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7df321-8b6c-425d-86da-9f03e419cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector = pd.DataFrame(columns = vectorizer.get_feature_names_out(), data = X.toarray()) \n",
    "text_vector #такие данные называются sparse data, об этом подробнее вы узнаете в курсе по машинному обучению"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b042da7-779c-41eb-b618-260c5c5a4f9a",
   "metadata": {},
   "source": [
    "Таким образом вы можете извлечь вектор любого интересующего вас текста в коллекции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e976c6-f7d4-44e5-bd36-de56e2761650",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector.loc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a84ae7-c3a1-4d89-8002-fbc8671e308a",
   "metadata": {},
   "source": [
    "И вектор любого интересующего слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cfdbda-05fe-4e2a-9001-36cb57fd699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_vector['онегин'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5ccb1-ee6d-4fa9-a77d-90ee0e2cc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_vector['татьяна'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a64e79-c344-4d93-bec2-d93f8dee29fe",
   "metadata": {},
   "source": [
    "**Задание 1**. Сделайте список из двух небольших текстов и получите их вектора."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f122a9d2-f2cf-42e6-8390-0f96b016137f",
   "metadata": {},
   "source": [
    "Посчитаем косинусное расстояние: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c009560-c56b-41e7-9da2-226b68ab068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478f4aa-f749-4d37-954d-af7ffafc3338",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_1 = [X.toarray()[1]]\n",
    "vector_2 = [X.toarray()[5]]\n",
    "cosine_similarity(vector_1, vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b3f06-786d-4d99-a043-5f5c8e599b0c",
   "metadata": {},
   "source": [
    "У векторных моделей широкое применение, особенно в машинном обучении, например, в задаче классификации текстов. Посмотрим на простой классификатор при помощи мешка слов.  \n",
    "**NB! От вас не требуется сейчас понимания и умения писать классификаторы, это только иллюстрация того, зачем вообще могут быть нужны векторные модели текста.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af044b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\Aleksandr\\\\Downloads\\\\archive\\\\spam.csv\", encoding='latin-1') #добавьте свой путь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35890e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847e7ff-7302-4a52-ac21-1a9bf49a58aa",
   "metadata": {},
   "source": [
    "Немного предобработакм датасет: удалим лишние столбцы и измененим метки спама."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c91d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#немного предобработки датасета: удаление лишних столбцов и изменение меток спама\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":\"label\", \"v2\":\"text\"})\n",
    "data['label'] = data.label.map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baac87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#количество спама и не-спама\n",
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb8705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5387ee5-14ec-46b8-a740-7abf6eb5e4ce",
   "metadata": {},
   "source": [
    "Также немного предобработаем наш корпус."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(data)): #проходимся по каждой строке\n",
    "    review = re.sub('[^a-zA-Z ]', '', data['text'][i].lower()) #удаляем все, что не латиница и не пробел\n",
    "    review = [ps.stem(word) for word in word_tokenize(review) if not word in set(stopwords.words('english'))] #стемминг\n",
    "    review = ' '.join(review) #склеиваем: CountVectorizer из sklearn требует строковых данных\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a548da16-04bf-4f58-a421-a2742004a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём модель мешка слов\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray() #полученные вектора\n",
    "y = data.iloc[:, 0].values #метки спама"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6bbb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))\n",
    "#print(len(X[1])) #количество типов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6e5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Naive Bayes to the Training set (Gaussian NB)\n",
    "prediction = dict()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "prediction[\"GaussianNB\"] = classifier.predict(X_test)\n",
    "accuracy_score(y_test,prediction[\"GaussianNB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90691f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание на мультиномиальном наивном байесовском алгоритме\n",
    "prediction[\"MultinomialNB\"] = classifier.predict(X_test)\n",
    "accuracy_score(y_test,prediction[\"MultinomialNB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, prediction['MultinomialNB'], target_names = [\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e78d6",
   "metadata": {},
   "source": [
    "<b>Поработаем с tf-idf.</b> Разберем для начала игрушечный пример."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e323f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = 'книга ужасно интересная'\n",
    "text_2 = 'фильм ужасно увлекательный'\n",
    "texts = [text_1, text_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184102b-b093-4696-ae1e-6a87f3ab4b95",
   "metadata": {},
   "source": [
    "Получим вектора токенов в этих текстах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35c032-4666-4f96-b8af-014a976be645",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1)) #ngram_range=(1, 1) означает униграммы, (1, 2) - униграммы и биграммы, (2, 2) - биграммы и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e05a3d-1ab5-41f2-b5f2-cea8a000a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "npm_tfidf = tfidf_matrix.todense()\n",
    "document_1_vector = npm_tfidf[0] #вектор первого текста\n",
    "document_2_vector = npm_tfidf[1] #вектор второго текста\n",
    "print(tfidf_vectorizer.get_feature_names_out()) #токены вектора\n",
    "print(document_1_vector)\n",
    "print(document_2_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = document_1_vector.tolist()\n",
    "x_2 = document_2_vector.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7013e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=[x_1[0], x_2[0]], columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477fe48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389a8c2-6c2b-44e9-9940-fcadc09e7e93",
   "metadata": {},
   "source": [
    "Вернемся к Пушкину."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c554c-8347-42d2-bd66-5e3ce3d3290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Aleksandr\\\\Downloads\\\\EugeneOnegin.txt', encoding='utf-8') as txt:\n",
    "    text = txt.read()\n",
    "    corpus = re.split(r'ГЛАВА \\w+\\b', text)\n",
    "    print(len(corpus))\n",
    "    clean_texts = []\n",
    "    for text in corpus:\n",
    "        text = re.sub(r'\\n', ' ', text)\n",
    "        text = re.sub('[^а-яА-ЯёЁ -]', '', text.lower())\n",
    "        lemmatized_text = [morph.parse(tok)[0].normal_form for tok in word_tokenize(text)]\n",
    "        text_no_stop = ' '.join([token for token in lemmatized_text if token not in stopwords_ru])\n",
    "        clean_texts.append(text_no_stop)\n",
    "print(clean_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e5a52-04f1-4ef0-9d1d-ed742c8c81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "X = tfidf_vectorizer.fit_transform(clean_texts) #полученные tf-idf вектора "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a10c91-88ad-4631-9a12-7eeeacafc994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af281a9b-578c-46ea-8226-42c0265c666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector = pd.DataFrame(columns = tfidf_vectorizer.get_feature_names_out(), data = X.toarray()) \n",
    "text_vector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9f383-8ba5-46ac-93c3-56195d031c38",
   "metadata": {},
   "source": [
    "И так же, как и с мешком слов, можем получать вектора отдельных слов и текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e759aa7-6dcb-4cb1-abb2-03f7d11aee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_vector['сплин'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b6034-7b35-4b6e-ae6a-88bebe1861ac",
   "metadata": {},
   "source": [
    "Извлечем ключевые слова для первой главы. Для начала объединим леммы со значениями tf-idf для первой главы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33989e4-2130-47d4-907f-39d7d2dac78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = list(text_vector.columns)\n",
    "tf_idf = text_vector.loc[0].tolist()\n",
    "lemmas_tf_idf = list(zip(lemmas, tf_idf))\n",
    "lemmas_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e98a3b-5dc7-40dc-8d9c-3c87a2c812bf",
   "metadata": {},
   "source": [
    "Осталось только отсортировать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643746b-9b80-452e-a426-f42ba6c3f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_chapter_one = sorted(lemmas_tf_idf, key=lambda x: x[1], reverse = 'True')\n",
    "sorted_chapter_one[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c69390a-ac33-4597-b365-e014dba1780d",
   "metadata": {},
   "source": [
    "Посмотрим на последнюю главу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46270c5-24b0-4751-8099-2f689f1aeb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = list(text_vector.columns)\n",
    "tf_idf = text_vector.loc[7].tolist()\n",
    "lemmas_tf_idf = list(zip(lemmas, tf_idf))\n",
    "sorted_chapter_eight = sorted(lemmas_tf_idf, key=lambda x: x[1], reverse = 'True')\n",
    "sorted_chapter_eight[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cbce88-a53d-4339-b9e2-bcdd3839c639",
   "metadata": {},
   "source": [
    "Результат, конечно, далек от совершенства. Отчасти потому, что это довольно искусственное разбиение на отдельные тексты (главы), отчасти потому, что мы могли бы добавить больше слоев предобработки. Например, оставив только существительные. Попробуем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f0125b-41e8-455f-a762-34a4800d762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Aleksandr\\\\Downloads\\\\EugeneOnegin.txt', encoding='utf-8') as txt:\n",
    "    text = txt.read()\n",
    "    corpus = re.split(r'ГЛАВА \\w+\\b', text)\n",
    "    print(len(corpus))\n",
    "    clean_texts = []\n",
    "    for text in corpus:\n",
    "        text = re.sub(r'\\n', ' ', text)\n",
    "        text = re.sub('[^а-яА-ЯёЁ -]', '', text.lower())\n",
    "        #чуть-чуть изменим предобработку, добавив одно условие:\n",
    "        lemmatized_text = [morph.parse(tok)[0].normal_form for tok in word_tokenize(text) if morph.parse(tok)[0].tag.POS == 'ADJF']\n",
    "        text_no_stop = ' '.join([token for token in lemmatized_text if token not in stopwords_ru])\n",
    "        clean_texts.append(text_no_stop)\n",
    "print(clean_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc4149-1a9b-49c3-a6fb-6b4b56ab3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_vectorizer.fit_transform(clean_texts) #полученные tf-idf вектора \n",
    "text_vector = pd.DataFrame(columns = tfidf_vectorizer.get_feature_names_out(), data = X.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919bb634-fe20-4910-ba96-f7e9e42d07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = list(text_vector.columns)\n",
    "tf_idf_one = text_vector.loc[0].tolist()\n",
    "tf_idf_eight = text_vector.loc[7].tolist()\n",
    "lemmas_tf_idf_one = list(zip(lemmas, tf_idf_one))\n",
    "lemmas_tf_idf_eight = list(zip(lemmas, tf_idf_eight))\n",
    "sorted_chapter_one = sorted(lemmas_tf_idf_one, key=lambda x: x[1], reverse = True)\n",
    "sorted_chapter_eight = sorted(lemmas_tf_idf_eight, key=lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5db43-bb81-45c6-824d-6870d690ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_chapter_one[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943b9e6-5df4-4245-b6cf-5b398347529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_chapter_eight[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf8226-6029-4eec-9291-bb0174400520",
   "metadata": {},
   "source": [
    "**Задание 1**. Сравните ключевые слова во второй главе (дружба Онегина и Ленского) и в шестой главе (убийство Ленского Онегиным)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1c730-a16f-492f-9201-63582af6ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = list(text_vector.columns)\n",
    "tf_idf_two = text_vector.loc[1].tolist()\n",
    "tf_idf_six = text_vector.loc[5].tolist()\n",
    "lemmas_tf_idf_two = list(zip(lemmas, tf_idf_two))\n",
    "lemmas_tf_idf_six = list(zip(lemmas, tf_idf_six))\n",
    "sorted_chapter_two = sorted(lemmas_tf_idf_two, key=lambda x: x[1], reverse = True)\n",
    "sorted_chapter_six = sorted(lemmas_tf_idf_six, key=lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5a1bc9-f44a-4b52-9968-b783f5eaef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_chapter_two[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c6f70-6dfd-4869-b11a-4b186f4c1138",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_chapter_six[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d288d-5fe3-4c32-a1fc-3392acb48a97",
   "metadata": {},
   "source": [
    "**Задание 2**. Соберите свой небольшой корпус из 3-4 текстов (например, 3-4 разные статьи из википедии на различные темы). Постройте tf-idf модель и сравните ключевые слова."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf264c1-534c-4f59-a5c8-cbacf2953146",
   "metadata": {},
   "source": [
    "**Задание 3 (со звездочкой)**. Модифицируйте код для предыдущего задания так, чтобы в ключевых словах были только глаголы. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
