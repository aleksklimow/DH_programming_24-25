{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f99a0c7d-3d8b-4fec-8bc0-8a3fc16cb791",
   "metadata": {},
   "source": [
    "**I. Синтаксис**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da014972-f106-4b17-a335-d86ebb244b0a",
   "metadata": {},
   "source": [
    "Синтаксическую разметку продуктивнее всего получать с помощью библиотек **stanza** или **spacy** (spacy вы можете рассмотреть самостоятельно: https://spacy.io/usage). В них много разных моделей для различных языков и достаточно понятное извлечение тэгов. Продолжим работу с stanza, c которой мы уже знакомы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be938b4-7198-4643-80b5-fcad9aba1e77",
   "metadata": {},
   "source": [
    "Если эта библиотека ещё не установлена, установите: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4552c36b-5ec2-4ca8-b1aa-7b001b5d6cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59624f52-613c-48d4-a0e7-228cb17c2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download(\"ru\")\n",
    "nlp_stanza = stanza.Pipeline(lang=\"ru\", processors=\"tokenize, pos, lemma, depparse, ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f16f8d-c57d-48de-8525-603f126a27ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Компания Газпром любит красивых кошек.' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95281d-3f48-4e52-97e7-56b23d9fceb7",
   "metadata": {},
   "source": [
    "Разметим наше предложение и посмотрим на результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b82b4a3-2cb9-4113-a941-8d1f353d23ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_stanza(text)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df87f8-bf24-4961-a739-fe886477abd1",
   "metadata": {},
   "source": [
    "Из этого мы можем аккуратно собрать всю морфологическую и синтаксическую информацию. В stanza мы работаем сначала с каждым предложением индивидуально:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdd4cc-ddca-451e-848b-05eede2f7aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385de4fe-24d2-4747-9feb-1b7b45300e47",
   "metadata": {},
   "source": [
    "А затем со словами: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5fbfd-9d48-475e-957c-3ce18bfa9357",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3890f5-813e-472e-8776-16c821beb9b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470b858d-a168-4a7e-9a36-cb999c26f97a",
   "metadata": {},
   "source": [
    "Отсюда уже можно по ключу получит нужную информацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d522a95-5ca2-482f-a17c-27b805b93eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(word.id, word.text, word.upos, word.deprel, word.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6849f1-93fd-4861-a46f-4546e3481053",
   "metadata": {},
   "source": [
    "Если, например, наша задача заключается в том, чтобы отследить какое количество разных типов подлежащих использует пишущий текст автор, мы должны использовать условные конструкции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05cd291-0819-48cc-baee-fc08d6c4b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Она любит зеленые кактусы. Он не любит зеленых камней. Газпром приносит счастье в дом. Он отчаянаная голова'\n",
    "doc = nlp_stanza(text)\n",
    "counter_pron = 0\n",
    "counter_noun = 0\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        if word.upos == 'PRON' and word.deprel == 'nsubj':\n",
    "            counter_pron += 1\n",
    "        elif word.upos == 'NOUN' and word.deprel == 'nsubj':\n",
    "            counter_noun += 1\n",
    "print(counter_pron)\n",
    "print(counter_noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c5c40-769c-41ac-b106-511b4f2e0530",
   "metadata": {},
   "source": [
    "Теперь склеим это в удобочитаемую табличку: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0595c0-d960-490e-a39f-d39d98dc6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a12ec-388e-4f1f-9bb9-657c22e50b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Я люблю красивых кошек.' \n",
    "doc = nlp_stanza(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f809eb-470b-4767-9c93-0d8402580d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_rows = [[word.id, word.text, word.upos, word.deprel, word.head, sentence.words[word.head-1].text] for sentence in doc.sentences for word in sentence.words]\n",
    "#это краткая запись цикла выше: не пугайтесь!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296822c-0600-42ed-851a-16438917894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4c6c2-6c7c-408b-97a9-769782b432b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence = pd.DataFrame(list_of_rows, columns=['id', 'token', 'pos', 'synt_tag', 'head_id', 'head_tok'])\n",
    "df_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3366759-67bd-4747-a63b-e2096b78edc0",
   "metadata": {},
   "source": [
    "А дальнейшая работа с подсчетом статистик - дело техники!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245eab7-7265-4c83-b7e9-207de3e7903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence.loc[df_sentence['synt_tag'] == 'nsubj']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cfc975-5328-4324-9901-2b9922a60ccb",
   "metadata": {},
   "source": [
    "Список возможных тэгов, указывающих на тип зависимости здесь: https://universaldependencies.org/u/dep/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2eed76-ff1c-4205-ae31-36bcc12aa171",
   "metadata": {},
   "source": [
    "Столбец head обозначает, какое слово является вершиной конкретного слова. Например, в предложении выше алгоритм считает, что вершиной предложения является \"люблю\" (root), вершиной \"кошек\" - слово \"люблю\". Т.о., у нас в этом коротком предложении есть несколько групп:  \n",
    "**группа глагола** (люблю -> я, люблю -> кошек, люблю -> .) и  \n",
    "**группа существительного** (кошек -> красивых)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac707d-f9ce-4626-8fd8-6fc611a14d42",
   "metadata": {},
   "source": [
    "Чуть-чуть изменим предложение выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ed10a-1c2a-405e-a7dc-6f327622da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_two = 'Юный чтец любит древние книги, которые находит в библиотеке.'\n",
    "doc = nlp_stanza(text_two)\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c39976-8c4f-4bec-84de-308011152635",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_rows = [[word.id, word.text, word.upos, word.deprel, word.head, sentence.words[word.head-1].text] for sentence in doc.sentences for word in sentence.words]\n",
    "df_sentence = pd.DataFrame(list_of_rows, columns=['id', 'token', 'pos', 'synt_tag', 'head_id', 'head_tok'])\n",
    "df_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275b2ed-7173-4a16-949b-e7960f52530c",
   "metadata": {},
   "source": [
    "Глазами мы видим следующие группы:  \n",
    "1. группа глагола главного предложения: любит -> чтец (**nsubj**, отношение сказуемого и подлежащего), любит -> книги (**obj**, отношение дополнения), любит -> . (**punct**, отношение пунктуации).\n",
    "2. группа подлежащего главного предложения: чтец -> юный (**amod**, отношение **adjectival modifier**, определение).\n",
    "3. группа прямого дополнения глагола главного предложения: книги -> древние (**amod**, определение), книги -> находит (**acl:relcl**, здесь сложнее. acl значит clausal modifier of noun, приименная клауза, которая при этом имеет надстройку в виде relcl - relative clause, относительное придаточное; алгоритм строит дерево не от существительного к относительному союзу, а к вершине зависимого предложения, но в самом отношении между существительным и зависимой вершиной предложения модифицируется для указания на тип придаточного предложения).\n",
    "4. группа глагола зависимого предложения: находит -> которые (nsubj, наше относительное местоимение является подлежащим зависимой клаузы), находит -> библиотеке (**obl**, oblique - т.е. зависимое от глагола существительное в косвенном падеже, не являющееся прямым дополнением), находит ->  , (**punct**).\n",
    "5. группа зависимого существительного в придаточном: библиотеке -> в (**case** нужен для указаний отношений существительного с предлогом, когда предлог требует какого-то косвенного падежда)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4447160-5b74-4b65-a782-818ccead0d5c",
   "metadata": {},
   "source": [
    "**Задание 1.** Выберите собственное предложение, оберните его в размеченную табличку и посмотрите, какие группы есть в этом предложении, разберите его так же, как в тексте выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432ac7f-d173-43c9-9ce0-d7975178153c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d769630e-8119-46e3-aa52-43fd3c9de234",
   "metadata": {},
   "source": [
    "На самом деле мы хотели бы собирать все группы не вручную, а автоматически."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd191184-20cd-4bc9-9a90-1eba45aaf74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Юный чтец любит древние книги, которые находит в библиотеке.'\n",
    "doc = nlp_stanza(text)\n",
    "list_of_rows = [[word.id, word.text, word.upos, word.deprel, word.head, sentence.words[word.head-1].text] for sentence in doc.sentences for word in sentence.words]\n",
    "list_of_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb65f22-d7b7-485a-bf22-8fb227ea1e86",
   "metadata": {},
   "source": [
    "Не самое эффективное, но достаточно действенное решение проблемы со сбором групп:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba1e55-e05c-4020-b819-3b6e84eea810",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Юный чтец любит древние книги, которые находит в библиотеке.'\n",
    "doc = nlp_stanza(text)\n",
    "list_of_rows = [[word.id, word.text, word.upos, word.deprel, word.head, sentence.words[word.head-1].text] for word in sentence.words for sentence in doc.sentences]\n",
    "df_sentence = pd.DataFrame(list_of_rows, columns=['id', 'token', 'pos', 'synt_tag', 'head_id', 'head_tok'])\n",
    "\n",
    "all_deps = [] #cюда мы сохраним пары вершина+зависимые\n",
    "for token in list_of_rows: #будем итерироваться по токенам\n",
    "    #print(token[5])\n",
    "    rslt_dep = df_sentence.loc[df_sentence['head_tok'] == token[5]] #нас интересует только 5-й элемент списка, это токен вершины\n",
    "    #print(rslt_dep)   \n",
    "    dep = list(rslt_dep['token'].values) #получим значение столбца\n",
    "    if (token[5], dep, len(dep)) not in all_deps: #если ещё нет такой вершины с зависимыми\n",
    "        all_deps.append((token[5], dep, len(dep))) #сохраним их в all_deps\n",
    "print(all_deps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0b6fd-d67c-43f5-acad-a1f121386f95",
   "metadata": {},
   "source": [
    "Посчитаем среднее количество зависимых в каждой из групп:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2ac8a-84c2-4e09-ac4d-d0036368e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for element in all_deps:\n",
    "    counter += element[2]\n",
    "avg_group = counter / len(all_deps)\n",
    "avg_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef356597-54f7-4316-adf9-69b728abb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = 'Екатерининская эпоха ознаменовалась максимальным закрепощением крестьян и всесторонним расширением привилегий дворянства.'\n",
    "doc = nlp_stanza(text_1)\n",
    "list_of_rows = [[word.id, word.text, word.upos, word.deprel, word.head, sentence.words[word.head-1].text] for sentence in doc.sentences for word in sentence.words]\n",
    "df_sentence = pd.DataFrame(list_of_rows, columns=['id', 'token', 'pos', 'synt_tag', 'head_id', 'head_tok'])\n",
    "\n",
    "all_deps_1 = [] #cюда мы сохраним пары вершина+зависимые\n",
    "for token in list_of_rows: #будем итерироваться по токенам\n",
    "    #print(token[5])\n",
    "    rslt_dep = df_sentence.loc[df_sentence['head_tok'] == token[5]] #нас интересует только 5-й элемент списка, это токен вершины\n",
    "    #print(rslt_dep)   \n",
    "    dep = list(rslt_dep['token'].values) #получим значение столбца\n",
    "    if (token[5], dep, len(dep)) not in all_deps: #если ещё нет такой вершины с зависимыми\n",
    "        all_deps_1.append((token[5], dep, len(dep))) #сохраним их в all_deps\n",
    "\n",
    "text_2 = 'У меня большая семья из шести человек: я, мама, папа, старшая сестра, бабушка и дедушка.'\n",
    "doc = nlp_stanza(text_2)\n",
    "list_of_rows = [[word.id, word.text, word.upos, word.deprel, word.head, sentence.words[word.head-1].text] for sentence in doc.sentences for word in sentence.words]\n",
    "df_sentence = pd.DataFrame(list_of_rows, columns=['id', 'token', 'pos', 'synt_tag', 'head_id', 'head_tok'])\n",
    "all_deps_2 = [] #cюда мы сохраним пары вершина+зависимые\n",
    "for token in list_of_rows: #будем итерироваться по токенам\n",
    "    #print(token[5])\n",
    "    rslt_dep = df_sentence.loc[df_sentence['head_tok'] == token[5]] #нас интересует только 5-й элемент списка, это токен вершины\n",
    "    #print(rslt_dep)   \n",
    "    dep = list(rslt_dep['token'].values) #получим значение столбца\n",
    "    if (token[5], dep, len(dep)) not in all_deps: #если ещё нет такой вершины с зависимыми\n",
    "        all_deps_2.append((token[5], dep, len(dep))) #сохраним их в all_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe49e71-b547-4273-8b58-bd219a518857",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_1 = 0\n",
    "for element in all_deps_1:\n",
    "    counter_1 += element[2]\n",
    "avg_group_1 = counter_1 / len(all_deps_1)\n",
    "avg_group_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354ac86-fb3f-4fb6-9508-af25e7436f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_2 = 0\n",
    "for element in all_deps_2:\n",
    "    counter_2 += element[2]\n",
    "avg_group_2 = counter_2 / len(all_deps_2)\n",
    "avg_group_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43bfcc1-59f6-4d10-835f-4bc5fe9f60a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Количество групп в нашем предложении:', len(all_deps)-1) #вычитаем единицу, потому отношение root лишнее"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f219d25b-024e-45ca-859d-e478ec6a4002",
   "metadata": {},
   "source": [
    "А если у нас несколько предложений в тексте?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd35dec-816b-4c6a-a92f-333cf24f8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Юный чтец любит древние книги, которые находит в библиотеке. Этот чтец является большим экспертом.'\n",
    "doc = nlp_stanza(text)\n",
    "list_of_rows = [[word.id, word.text, word.upos, word.deprel, word.head, sentence.words[word.head-1].text] for sentence in doc.sentences for word in sentence.words]\n",
    "df_sentence = pd.DataFrame(list_of_rows, columns=['id', 'token', 'pos', 'synt_tag', 'head_id', 'head_tok'])\n",
    "df_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0882d4c-d21b-4eae-99ab-e32e10797029",
   "metadata": {},
   "source": [
    "Будет ошибка. Это потому что stanza разбирает текст по предложениям. Поправим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e0bc4-25b4-4dba-abba-d1ac14dc46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_rows = []\n",
    "counter = 0\n",
    "for sentence in doc.sentences:\n",
    "    counter += 1  \n",
    "    for word in sentence.words:\n",
    "       # print([word.id, word.text, word.upos, word.deprel, word.head, sentence.words[word.head-1].text])\n",
    "        list_of_rows.append([counter, word.id, word.text, word.upos, word.deprel, word.head, sentence.words[word.head-1].text])\n",
    "\n",
    "df_sentence = pd.DataFrame(list_of_rows, columns=['sent_id', 'id', 'token', 'pos', 'synt_tag', 'head_id', 'head_tok'])\n",
    "df_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d95319-8c86-419b-a24f-301bd676e2a1",
   "metadata": {},
   "source": [
    "Если мы оставим предыдущий код для подсчета зависимых, то он будет ошибаться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b6fff-425a-4761-bf14-3e7c0d44f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deps = [] #cюда мы сохраним пары вершина+зависимые\n",
    "for token in list_of_rows: #будем итерироваться по токенам\n",
    "   # print(token[6])\n",
    "    rslt_dep = df_sentence.loc[df_sentence['head_tok'] == token[6]] #нас интересует только 5-й элемент списка, это токен вершины\n",
    "    dep = list(rslt_dep['token'].values) #получим значение столбца\n",
    "    if (token[6], dep) not in all_deps: #если ещё нет такой вершины с зависимыми\n",
    "        all_deps.append((token[6], dep)) #сохраним их в all_deps\n",
    "print(all_deps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc43229b-9ceb-4cd6-8a2d-0102a0f749c4",
   "metadata": {},
   "source": [
    "Нужно ввести контроль, чтобы мы работали внутри предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e76d7-d32d-461d-bc0b-57763c53c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deps = [] #cюда мы сохраним пары вершина+зависимые\n",
    "for token in list_of_rows: #будем итерироваться по токенам\n",
    "   # print(token[6])\n",
    "    rslt_dep = df_sentence.loc[(df_sentence['head_tok'] == token[6]) & (df_sentence['sent_id'] == token[0])]  #нас интересует только 5-й элемент списка, это токен вершины\n",
    "    dep = list(rslt_dep['token'].values) #получим значение столбца\n",
    "    if (token[6], dep, len(dep)) not in all_deps: #если ещё нет такой вершины с зависимыми\n",
    "        all_deps.append((token[6], dep, len(dep))) #сохраним их в all_deps\n",
    "print(all_deps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b725b2-f9a1-4365-9b6b-77a2ef1e7e13",
   "metadata": {},
   "source": [
    "Ура! Для полноты картины в принципе можно делить тексты по предложениям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe808fb-1b10-4564-851c-a38d18eeb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deps = [] #cюда мы сохраним пары вершина+зависимые\n",
    "for token in list_of_rows: #будем итерироваться по токенам\n",
    "   # print(token[6])\n",
    "    rslt_dep = df_sentence.loc[(df_sentence['head_tok'] == token[6]) & (df_sentence['sent_id'] == token[0])]  #нас интересует только 5-й элемент списка, это токен вершины\n",
    "    dep = list(rslt_dep['token'].values) #получим значение столбца\n",
    "    if (token[0], token[6], dep, len(dep)) not in all_deps: #если ещё нет такой вершины с зависимыми\n",
    "        all_deps.append((token[0], token[6], dep, len(dep))) #сохраним их в all_deps\n",
    "\n",
    "for deps in all_deps:\n",
    "    print(deps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e4ba1-de29-460f-b6a5-e9c532aee68f",
   "metadata": {},
   "source": [
    "Количество групп в предложении можно назвать **\"шириной\"** синтаксического дерева. Таким образом, в первом предложении текста ширина равна 5 (минус root, как мы помним), во втором - 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c4678-6eb7-490a-89ae-9334284ef431",
   "metadata": {},
   "source": [
    "**Задание 2**. Разметьте небольшой текст. Сколько в среднем зависимых у каждого из слов? Сколько в принципе групп в каждом предложении? Какое среднее значение групп по тексту?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8057bafb-4c0d-4b22-a9da-431f7703e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = 'Дочь князя Ангальт-Цербстского, Екатерина взошла на престол в результате дворцового переворота против своего мужа — Петра III, вскоре погибшего при невыясненных обстоятельствах (возможно, он был убит)[3]. Она взошла на престол, следуя прецеденту, созданному Екатериной I, сменившей своего умершего мужа Петра Великого в 1725 году. Екатерининская эпоха ознаменовалась максимальным закрепощением крестьян и всесторонним расширением привилегий дворянства. При Екатерине Великой границы Российской империи были значительно сдвинуты на запад (разделы Речи Посполитой) и на юг (присоединение Новороссии, Крыма, отчасти Кавказа). Были созданы условия для свободной деятельности всех конфессий; положение староверов (раскольников) было облегчено. Система государственного управления при Екатерине Второй впервые со времени Петра I была реформирована. Сенат был разделён на шесть департаментов, возглавляемых обер-прокурорами, возглавил Сенат генерал-прокурор. Общие полномочия Сената были сокращены: в частности, он лишился законодательной инициативы. Была проведена губернская реформа, в ходе которой было преобразовано провинциальное управление в наместничествах. Расходы на содержание чиновничьего аппарата резко возросли. Характерной особенностью правления Екатерины II стал фаворитизм, государственные расходы на фаворитов исчислялись десятками миллионов рублей. Повсеместными были коррупция и злоупотребления чиновников. На фоне происходившей в ряде других стран промышленной революции, в России использовался в основном ручной труд без развития механизации и применения новых технологий, поскольку Екатерина II считала, что машины наносят вред государству, сокращая численность работающих. В структуре экспорта совсем не было готовых изделий, только сырьё и полуфабрикаты, а 80—90 % импорта составляли зарубежные промышленные изделия. К концу правления Екатерины II Россия находилась в тяжёлом экономическом кризисе при полном крушении финансовой системы, общая сумма долгов правительства составляла 205 млн рублей. Внешние займы Екатерины II и начисленные на них проценты были полностью погашены только в 1891 году. У Екатерины II на всём протяжении её правления были десятки любовников, некоторые из которых оказывали большое влияние на внутреннюю и внешнюю политику. Распутство императрицы проявлялось в откровенно вызывающей форме и способствовало падению нравов дворянства. Екатерина II увлекалась литературной деятельностью, собирала шедевры живописи, состояла в переписке с французскими просветителями. Императрица предприняла ряд попыток преобразований в духе просвещённого абсолютизма, но эти преобразования имели ограниченный характер.'\n",
    "text_2 = 'Привет! Меня зовут Андрей! Мне 12 лет. У меня много друзей. Моего лучшего друга зовут Паша. Мы одного возраста и учимся в одном классе. Он живет в соседнем доме. Мы ходим в школу пешком вместе. У него короткие светлые волосы, зеленые глаза и добрая улыбка. Он очень худой и высокий. Паша выше меня на целую голову. Он часто смеется и готов помочь. Мы обычно играем в футбол с другими ребятами после школы. Мы всегда играем в одной команде. Паша очень любит спорт и мечтает стать тренером, когда вырастет. Иногда мы идем ко мне домой после футбола и делаем вместе домашнюю работу. Паша всегда объясняет мне сложные задачи, потому что учится лучше меня, особенно по математике.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbaab0c-bf1e-44c9-80e7-f158d58c3513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83f7286e-157f-448c-a000-a10f9ebaffb5",
   "metadata": {},
   "source": [
    "Итак, у нас есть размеченный корпус или текст. Например, что мы можем извлекать с помощью синтаксиса? Например, что именно синтаксически сочетается с конкретным словом. Вернемся к нашей табличке, но разметим что-нибудь другое:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d878621-792e-45c9-92f3-3477c76e4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Joshua.txt', encoding='utf-8') as txt:\n",
    "    text = txt.read()\n",
    "    doc = nlp_stanza(text[:50000])\n",
    "    list_of_rows = [[word.id, word.text, word.lemma, word.upos, word.deprel, word.head, sentence.words[word.head-1].text, sentence.words[word.head-1].lemma] for sentence in doc.sentences for word in sentence.words]\n",
    "    df_joshua = pd.DataFrame(list_of_rows, columns=['id', 'token', 'lemma','pos', 'synt_tag', 'head_id', 'head_tok', 'head_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c55fd-355e-4da0-8509-3aac2d8049d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joshua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f0a74-e2e6-4ed6-8f25-1886e6a54b5f",
   "metadata": {},
   "source": [
    "Давайте найдем теперь все адъективные модификаторы (amod) cлова \"море\". Сначала посмотрим просто на amod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67de65a-ce1b-404b-b46f-3dde949eae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joshua.loc[df_joshua['synt_tag'] == 'amod']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b89cd3a-ff12-4019-a6fd-3bc47bd443a2",
   "metadata": {},
   "source": [
    "Пока это только искомые слова. Но мы не зря вывели лемму вершины:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761efa4b-c833-43cb-996e-0c073db4729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joshua.loc[(df_joshua['synt_tag'] == 'amod') & (df_joshua['head_lemma'] == 'море')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44b353-c100-4705-a71b-eee0c6d15914",
   "metadata": {},
   "source": [
    "Теперь осталось посчитать, сколько раз каждый из модификаторов повторился:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eede940-8d68-4e0c-a456-115256d461e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joshua.loc[(df_joshua['synt_tag'] == 'amod') & (df_joshua['head_lemma'] == 'море')].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a8a36-ebf5-4850-813c-d36c8496c495",
   "metadata": {},
   "source": [
    "Увы, тут проверка на уникальность, а у нас как минимум меняются словоформы и вершины. Поправим так, чтобы учитывалась только лемма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ddea3-914e-4579-bdf4-96152b51b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amods = df_joshua.loc[(df_joshua['synt_tag'] == 'amod') & (df_joshua['head_lemma'] == 'море')]\n",
    "df_amods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22df29-bb27-4edc-8df8-bbb575124f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amods.groupby('lemma').size().reset_index(name='counts')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
