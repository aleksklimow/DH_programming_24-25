{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48339384-4a78-4d4c-bda5-31fd4f6c79dd",
   "metadata": {},
   "source": [
    " Сегодня поработаем с романом \"Sense and Sensibility\" Джейн Остен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dbccbb-b64e-43bb-8276-81d2d05dc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('SenseAndSensibilityJAu.txt', encoding='utf-8') as txt: #положите файл для обработки в корневую папку для вашего удобства\n",
    "    text = txt.read()\n",
    "    print(len(text.split())) #наивная токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6366b1-98fe-4263-9571-372926835eaf",
   "metadata": {},
   "source": [
    "Довольно большой роман, но на самом деле там много всякой ненужной нам информации, поэтому сделаем чуть больше предобработки, чем обычно. Проверим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8a4a1-c48f-4cf9-9d5d-b933c3fecb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:1550]) #cрез по первым 1550 символам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406799c-118c-4cc0-bc37-7b58fcb278c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[-19000:]) #19 тысяч символов с конца"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc640d-812c-4968-a41d-e2ed92a629f7",
   "metadata": {},
   "source": [
    "Давайте удалим эти вставки в начале и конце. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec0df0-5ae4-446c-9485-67f3fa339e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add27e44-c7bc-4ae3-8328-efecd5b5c548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_no_info = re.sub(r'\\n', ' ', text)\n",
    "text_no_info = re.sub('^The Project Gutenberg eBook of Sense and Sensibility.*?CHAPTER L', '', text_no_info)\n",
    "print(text_no_info[:1550])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d56cdc-631f-4196-81c8-dd853e43f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_no_info = re.sub(r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK SENSE AND SENSIBILITY \\*\\*\\*\\n.*', '', text_no_info)\n",
    "print(text_no_info[-19000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d20df-86d7-46e2-90ac-db41c4a4d909",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_no_info = re.sub(' +', ' ', text_no_info) #косметически уберем лишние пробельчики\n",
    "text_no_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b5117-0876-47b5-b16a-cbc073738d3e",
   "metadata": {},
   "source": [
    "Лемматизировать английский текст через pymorphy (как мы ранее делали с русскоязычным текстом) бесполезно. Можно, например, через nltk, stanza или другие инструменты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28576695-7581-42cc-bba7-a1a507f1d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b3230-7cac-4e01-b65a-64d4c1662bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text_no_info[:1000]) if word not in stopwords_en]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d10df-8918-4991-a59e-2c11a17753d2",
   "metadata": {},
   "source": [
    "Как вы видите, результат похож скорее на стемминг. Проверим stanza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c2e19-8a46-49c1-bb5b-3b5733bf80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "nlp_stanza = stanza.Pipeline(lang=\"en\", processors=\"tokenize, pos, lemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b93a19-ee0b-4cc1-a13d-89ac08e17412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1650a01c-431d-4818-a197-a298efd77344",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_stanza = nlp_stanza(text_no_info[:1000])\n",
    "print(text_stanza) #результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33196de4-9816-4d68-8b2d-138b8bf375b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_lemmas = []\n",
    "for sentence in text_stanza.iter_tokens():\n",
    "  #  print(sentence)\n",
    "    for word in sentence.words:\n",
    "        #print(word)\n",
    "        #print(word.lemma.lower())\n",
    "        word_lem = word.lemma.lower()\n",
    "        if word_lem not in stopwords_en:\n",
    "            full_text_lemmas.append(word_lem)\n",
    "       #     print('yay!')\n",
    "      #  else:\n",
    "         #   print ('nay')\n",
    "full_text_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d2c09-a805-4aba-9384-b03e7e5ac3dc",
   "metadata": {},
   "source": [
    "Это было скорее напоминание о stanza. Это довольно медленный инструмент и мы не можем пользоваться им на занятии, но для исследований, требующих высокой точности и при этом позволяющих оставить компьютер размечать некоторое время - это одно из лучших решений. Теперь вернемся к WordNetLemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a85b6e-1f26-49a8-a634-3515761976e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2998717f-2860-42c3-b42b-e88a8ac9a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_text_no_info = [lemmatizer.lemmatize(word) for word in tqdm(word_tokenize(text_no_info.lower())) if word not in stopwords_en]\n",
    "print(len(lem_text_no_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42862eed-2e98-4ab4-8c2a-bd9d7d0617bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_text_no_info[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b7f4c-38eb-407a-a9a4-894e398d0141",
   "metadata": {},
   "source": [
    "Нарисуем dispersion plot с помощью nltk, чтобы посмотреть распределения слов по графику (этот кусочек позаимствован у Д. Скоринкина). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fdfee6-3174-40d8-ad2b-4820b836e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Text as nltk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d265c-d4af-4249-b00c-5cc22e646c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_no_info_nltk = nltk_text(lem_text_no_info)\n",
    "type(text_no_info_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb85f7-b2b3-4e40-8024-07ee87898b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_nltk_format.dispersion_plot(['joy', 'sorrow', 'death', 'life', 'dead', 'happy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe58f98-e0a8-4a37-b73b-a1dd7093e028",
   "metadata": {},
   "source": [
    "Также мы можем самостоятельно разбить текст на квартили, децили или любые другие блоки (чанки) и посмотреть, насколько распределены те или иные единицы по корпусу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f027c2-bb95-4867-8d03-574dedeebcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_join = ' '.join(lem_text_no_info)\n",
    "length = len(text_join)\n",
    "chunk_size = length // 10 #количество блоков\n",
    "remainder = length % 10 #количество блоков\n",
    "chunks = []\n",
    "for i in range(9): #в скобочках нужно указать такое число, на которое вы хотите разделить текст, -1. Здесь мы делим на децили\n",
    "    chunks.append(text_join[i * chunk_size : (i + 1) * chunk_size])\n",
    "    chunks.append(text_join[9 * chunk_size:]) \n",
    "\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526f8fb-4bf3-4b0a-aae2-89ad5abc1a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(len(lem_text_no_info) / 10)) #длина чанков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e8d2f-ffbb-4b59-9860-75f96323f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "chunk_size = round(len(lem_text_no_info) / 10)  # Целочисленное деление для размера чанка\n",
    "chunk_beg = 0\n",
    "\n",
    "for i in range(10):\n",
    "    chunk_switch = chunk_beg + chunk_size\n",
    "    if i == 9:  # Последний чанк включает остаток\n",
    "        chunk_switch = len(lem_text_no_info)\n",
    "    chunk = lem_text_no_info[chunk_beg:chunk_switch]\n",
    "    chunks.append(' '.join(chunk))\n",
    "    chunk_beg = chunk_switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3d60bb-bc6f-4f81-b0c0-1bf06609985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks[7]))\n",
    "print(len(chunks[1]))\n",
    "print(len(chunks[9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a329f07-8b9c-4828-9354-aa317162b3d6",
   "metadata": {},
   "source": [
    "Проверим, что последний чанк действительно последний:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80013b7-7559-4dd1-8c79-344126dce5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec61a8d-142a-409a-b9ab-2ade7092c546",
   "metadata": {},
   "source": [
    "А вот теперь можно посчитать разную статистику по нашим децилям. Например, найдем количество слов 'sorrow' в каждом из чанков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f7394-d0ba-47c1-8a7b-0a4726b5b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8896289-0ddb-4662-ba73-0db97aa45bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_lady_of_sorrows = []\n",
    "for chunk in chunks:\n",
    "    counter = 0\n",
    "    chunk_tok = chunk.split()\n",
    "    for word in chunk_tok:\n",
    "        if word == 'sorrow':\n",
    "            counter += 1\n",
    "    our_lady_of_sorrows.append(counter)\n",
    "our_lady_of_sorrows    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d270531f-bb0c-4d07-8880-c7c3761424a1",
   "metadata": {},
   "source": [
    "Поместим это на график:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945675e8-77a6-47f1-b9cc-e8cf9c60262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af6ba2-4c81-4951-a7a7-3d8fee4aecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(our_lady_of_sorrows)\n",
    "plt.xlabel(\"Дециль книги\")\n",
    "plt.ylabel(\"Количество единиц слова\")\n",
    "plt.title(\"Sorrow\")\n",
    "plt.xticks(range(len(our_lady_of_sorrows))) #чтобы показывались все децили\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae07fe-0a56-465e-8d7b-2583c343ab8e",
   "metadata": {},
   "source": [
    "Этого нам мало. Давайте извлечен несколько слов и разобъем по децилям. Обернем наш код выше в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db2c63d-5ec9-4a22-b489-fba0d54abb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_chunk_counter(data, word_s):\n",
    "    word_in_chunks = []\n",
    "    for chunk in data:\n",
    "        counter = 0\n",
    "        chunk_tok = chunk.split()\n",
    "        for word in chunk_tok:\n",
    "            if word == str(word_s):\n",
    "                counter += 1\n",
    "        word_in_chunks.append(counter)\n",
    "    return word_in_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40036695-3f2c-454e-b5b5-f6c6f82d8f4c",
   "metadata": {},
   "source": [
    "Проверим для начала на 'sorrow':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bcea18-67e0-4d2a-b44f-75ea0913256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_lady_of_sorrows_2 = word_chunk_counter(chunks, 'sorrow')\n",
    "print(our_lady_of_sorrows == our_lady_of_sorrows_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792dd4d-3e31-4822-8189-ecd217c59112",
   "metadata": {},
   "source": [
    "Отлично, всё работает правильно! Найдем ещё 5 слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f814d-fe6a-47ed-910e-5d71a53e24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dolor = word_chunk_counter(chunks, 'sorrow')\n",
    "gaudium = word_chunk_counter(chunks, 'joy')\n",
    "mors = word_chunk_counter(chunks, 'death')\n",
    "nativitas = word_chunk_counter(chunks, 'birth')\n",
    "felix = word_chunk_counter(chunks, 'happy')\n",
    "tristus = word_chunk_counter(chunks, 'sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd7cf8-b73d-41a8-894d-a113218a59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dolor)\n",
    "print(gaudium)\n",
    "print(mors)\n",
    "print(nativitas)\n",
    "print(felix)\n",
    "print(tristus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e21a73-f8f0-4b7a-9a0c-26ae025ca3db",
   "metadata": {},
   "source": [
    "Нанесем их все на один график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f75eb-120e-447a-a46e-94b574c9bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [dolor, gaudium, mors, nativitas, felix, tristus]\n",
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4367d07-832f-4b0d-9d11-30e2224b763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(all_words[0])) # ось Х одинакова для всех линий\n",
    "line_names = ['sorrow', 'joy', 'death', 'birth', 'happy', 'sad']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "\n",
    "for i, line_data in enumerate(all_words):\n",
    "    sns.lineplot(x=x, y=line_data, label=line_names[i], marker='o')\n",
    "\n",
    "plt.xlabel(\"Дециль книги\", fontsize=12)\n",
    "plt.ylabel(\"Количество единиц слова\", fontsize=12)\n",
    "plt.title(\"Распределение частот шести заданных слов\", fontsize=14)\n",
    "plt.legend() # показываем легенду\n",
    "plt.xticks(range(len(our_lady_of_sorrows)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56da47-f819-4e1c-8f2f-22c93de3af51",
   "metadata": {},
   "source": [
    "Также иногда делают один график, но со множеством разных ячеек-графиков, получается что-то вроде плитки шоколада."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdee8fa-1d50-46b9-b5c9-219273962740",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(all_words[0]))\n",
    "line_names = ['sorrow', 'joy', 'death', 'birth', 'happy', 'sad']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8)) \n",
    "axes = axes.flatten()\n",
    "\n",
    "sns.set_theme(style=\"ticks\")\n",
    "colors = sns.color_palette(\"plasma\", len(all_words))\n",
    "\n",
    "for i, line_data in enumerate(all_words):\n",
    "    ax = axes[i]\n",
    "    sns.lineplot(x=x, y=line_data, marker='o', ax=ax, color=colors[i]) \n",
    "    ax.set_title(line_names[i]) \n",
    "    ax.set_xlabel(\"Дециль книги\")\n",
    "    ax.set_ylabel(\"Количество единиц слова\")\n",
    "    ax.set_xticks(range(len(our_lady_of_sorrows)))\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.suptitle(\"Распределение исследуемых слов\", fontsize=16)\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052089f-078e-4c79-bdb8-ede9abee60fb",
   "metadata": {},
   "source": [
    "**Задание 1.** Поэксперементируйте с другими словами. Какой результат у вас получается?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06196b05-6637-4f40-aa0f-5bf700965704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ac365dc-27d1-460a-af54-a82f4f675df7",
   "metadata": {},
   "source": [
    "**Задание 2.** Переделайте исследование так, чтобы считались не абсолютные значения, а относительные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc01c8-bf72-46a4-a555-370eb723e664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e226a9b-f963-4ce0-97ba-8a51cf71c740",
   "metadata": {},
   "source": [
    "Вернемся в сладостные объятия русского языка. Поисследуем Библию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262988f-0aca-4e3c-b3aa-fdece8caba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Torah.txt', encoding='utf-8') as txt:\n",
    "    text_torah = txt.read()\n",
    "    text_torah = re.sub(r'\\n', ' ', text_torah[:1000])\n",
    "    text_torah = re.sub(r'[^а-яА-Я -]', '', text_torah)\n",
    "    text_torah = re.sub(r' +', ' ', text_torah)\n",
    "    print(text_torah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c242161-e307-4e67-99b2-10e47268cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Torah.txt', encoding='utf-8') as txt:\n",
    "    text_torah = txt.read()\n",
    "    text_torah = re.sub(r'\\n', ' ', text_torah)\n",
    "    text_torah = re.sub(r'[^а-яА-Я -]', '', text_torah)\n",
    "    text_torah = re.sub(r' +', ' ', text_torah)\n",
    "\n",
    "with open('Joshua.txt', encoding='utf-8') as txt:\n",
    "    text_joshua = txt.read()\n",
    "    text_joshua = re.sub(r'\\n', ' ', text_joshua)\n",
    "    text_joshua = re.sub(r'[^а-яА-Я -]', '', text_joshua)\n",
    "    text_joshua = re.sub(r' +', ' ', text_joshua)\n",
    "\n",
    "with open('New_testament.txt', encoding='utf-8') as txt:\n",
    "    text_nt = txt.read()\n",
    "    text_nt = re.sub(r'\\n', ' ', text_nt)\n",
    "    text_nt = re.sub(r'[^а-яА-Я -]', '', text_nt)\n",
    "    text_nt = re.sub(r' +', ' ', text_nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78215a-652d-495c-844e-2d04cf0bc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text_torah.split()))\n",
    "print(len(text_joshua.split()))\n",
    "print(len(text_nt.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ff1d8-8537-42b0-9935-be2f142ce3ab",
   "metadata": {},
   "source": [
    "На децили разбивать не будем, просто сравним три книги по употреблению лексем и граммем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfdfd6a-1096-475b-8af9-e62b01ba6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy3 import MorphAnalyzer #загрузим анализатор\n",
    "morph = MorphAnalyzer() #сохраним его в переменную, чтобы заработал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1affa6d6-ca73-4a47-aabd-95fd9da64642",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_joshua = []\n",
    "for word in word_tokenize(text_joshua):\n",
    "    parsed_word = morph.parse(word)[0]\n",
    "    if parsed_word.tag.POS == 'VERB':\n",
    "        tagged_joshua.append((parsed_word.normal_form, parsed_word.tag.tense))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb22ab-9d37-4bb3-b17f-4d7e7eb9fb49",
   "metadata": {},
   "source": [
    "Можно переделать это в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b54723-0284-48da-bb7b-22d49e6bcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger_text(data, tag):\n",
    "    tagged_list = []\n",
    "    for word in word_tokenize(data):\n",
    "        parsed_word = morph.parse(word)[0]\n",
    "        if parsed_word.tag.POS == str(tag):\n",
    "            tagged_list.append((parsed_word.normal_form, parsed_word.tag.tense))\n",
    "    return tagged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb29113-863a-4e69-ac87-27b2311320f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_joshua_2 = tagger_text(text_joshua, 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137ec94-e66d-4ca3-8b52-a5364cdb349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_joshua == tagged_joshua_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c7802b-cfe6-4f5e-9d93-44c73dfca331",
   "metadata": {},
   "source": [
    "Отлично. Затегируем две оставшиеся книги:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1f151-2adc-492f-86b7-dbb4739e5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_torah = tagger_text(text_torah, 'VERB')\n",
    "tagged_nt = tagger_text(text_nt, 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc19e2-2013-4dbb-bb83-4478f23f6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_nt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c3d17-ed76-41db-80a0-d028e99afb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835e98f-2a4c-4795-a77b-02e1ef12485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_lemmas_torah = [item[0] for item in tagged_torah]\n",
    "count = collections.Counter(verb_lemmas_torah)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d00f763-b6ed-42ac-8779-252e760ea0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_frequency_list = []\n",
    "for item, freq in tqdm(count.items()):\n",
    "    rel_freq = freq / len(tagged_torah) * 100\n",
    "    rel_frequency_list.append((item, freq, rel_freq))\n",
    "rel_frequency_list.sort(key=lambda x: x[1], reverse=True)\n",
    "rel_frequency_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6225d8f-885b-41a5-82c7-7ed46eac3ec2",
   "metadata": {},
   "source": [
    "Обернем в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96f371-1bee-4c15-8936-77dce2cb3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_freq_list(data):\n",
    "    verb_lemmas = [item[0] for item in data]\n",
    "    count = collections.Counter(verb_lemmas)\n",
    "    rel_frequency_list = []\n",
    "    for item, freq in tqdm(count.items()):\n",
    "        rel_freq = freq / len(tagged_torah) * 100\n",
    "        rel_frequency_list.append((item, freq, rel_freq))\n",
    "    rel_frequency_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    return rel_frequency_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117559b-f9e7-4d6b-8163-9e6e4cd7d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_freq_list(tagged_torah) == rel_frequency_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5cbed7-d855-468f-bfc1-f4cffafbd14c",
   "metadata": {},
   "source": [
    "Посмотрим на списки двух других частей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68db543-1c98-41b9-b5c8-e17bf1545169",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_freq_list(tagged_joshua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa49df-973f-4c5a-a848-cd90a45b67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_freq_list(tagged_nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a05003b-9c2b-49c5-8b30-fe79fd1f7c9b",
   "metadata": {},
   "source": [
    "Можно визуализировать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b81e883-3c21-4139-a887-9334abd997ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [item for item, freq, rel_freq in rel_freq_list(tagged_nt)[:30]]\n",
    "frequencies = [rel_freq for item, freq, rel_freq in rel_freq_list(tagged_nt)[:30]]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(items, frequencies)\n",
    "plt.xlabel(\"Глагол\")\n",
    "plt.ylabel(\"Частота глагола\")\n",
    "plt.title(\"Частотное распределение глаголов\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec5f6a-9539-4f03-8d85-e39d2d171f88",
   "metadata": {},
   "source": [
    "Посчитаем относительное (относительного глаголов, а не всех токенов) количество слов \"убить\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01463215-e5a5-4de5-b858-491ef11cc5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books = [tagged_torah, tagged_joshua, tagged_nt]\n",
    "counters = []\n",
    "for book in tqdm(all_books):\n",
    "    num_verbs = len(book)\n",
    "    counter = 0\n",
    "    for word in book:\n",
    "        if word[0] == 'убить':\n",
    "            counter += 1\n",
    "    counters.append(counter / num_verbs * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564efdac-fc12-4fd1-8d67-8735f416768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248369d-37d2-4433-a392-a173926b21be",
   "metadata": {},
   "source": [
    "И снова обернем в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7fe00-9d59-466f-99d1-82eee9fa6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bible_token_rel_counter(data, token):\n",
    "    counters_tok = []\n",
    "    for book in tqdm(all_books):\n",
    "        num_verbs = len(book)\n",
    "        counter = 0\n",
    "        for word in book:\n",
    "            if word[0] == str(token):\n",
    "                counter += 1\n",
    "        counters_tok.append(counter / num_verbs * 100)\n",
    "    return (counters_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dabe487-1d93-4f9d-8092-c20623754ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_token_rel_counter(all_books, 'убить') == counters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f0aa67-3a83-418b-bcae-62bfcd144453",
   "metadata": {},
   "source": [
    "Отлично! Получим подсчёты слов \"родить\" и \"миловать\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd6068-44f3-4280-a750-54f69e5a7124",
   "metadata": {},
   "outputs": [],
   "source": [
    "kill = bible_token_rel_counter(all_books, 'убить')\n",
    "born = bible_token_rel_counter(all_books, 'родить')\n",
    "spare = bible_token_rel_counter(all_books, 'прощать')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec5b53-c838-40ae-a8a5-7a8f9b44526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "born"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b9ff1-d33d-4f50-932a-e14167ac4717",
   "metadata": {},
   "outputs": [],
   "source": [
    "spare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced013d8-f5fe-4514-b903-b61925cc13e2",
   "metadata": {},
   "source": [
    "Теперь нанесем все три слова на график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ea5cf-cf47-4c1e-9a60-8d370a3e3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [kill, born, spare]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2459ff-6359-4e09-a0e7-9c683ee4830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(words[0]))\n",
    "line_names = ['убить', 'родить', 'прощать']\n",
    "book_names = ['Тора', 'Книга Иисуса Навина', 'Новый Завет']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "\n",
    "for i, line_data in enumerate(words):\n",
    "    sns.lineplot(x=x, y=line_data, label=line_names[i], marker='o')\n",
    "\n",
    "plt.xlabel(\"Книга библии\", fontsize=12)\n",
    "plt.ylabel(\"Относительная частота\", fontsize=12)\n",
    "plt.title(\"Распределение частот слов\", fontsize=14)\n",
    "plt.legend() # показываем легенду\n",
    "plt.xticks(x, book_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c610b7-e732-4f41-af53-294f1cd83951",
   "metadata": {},
   "source": [
    "И на трех разных графиках:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd29af4-6600-496b-8bfa-81bccd26a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(words[0]))\n",
    "line_names = ['убить', 'родить', 'прощать']\n",
    "book_names = ['Тора', 'Книга Иисуса Навина', 'Новый Завет']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,5)) \n",
    "axes = axes.flatten()\n",
    "\n",
    "sns.set_theme(style=\"ticks\")\n",
    "colors = sns.color_palette(\"plasma\", len(words))\n",
    "\n",
    "for i, line_data in enumerate(words):\n",
    "    ax = axes[i]\n",
    "    sns.lineplot(x=x, y=line_data, marker='o', ax=ax, color=colors[i]) \n",
    "    ax.set_title(line_names[i]) \n",
    "    ax.set_xlabel(\"Книга библии\")\n",
    "    ax.set_ylabel(\"Относительная частота\")\n",
    "    ax.set_xticks(x, book_names)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.suptitle(\"Распределение частот слов\", fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00897883-1b5e-4991-b663-73936f1a2ad5",
   "metadata": {},
   "source": [
    "**Задание 3.** Измените код выше так, чтобы он извлекал существительные. Исследуйте несколько значимых (на ваш взгляд) существительных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40af3b98-c873-4065-9d45-8ab12b9a3771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0565446e-8be3-48e3-8681-6feb6bd954f0",
   "metadata": {},
   "source": [
    "Изучим граммемы. Давайте нанесем на график глаголы прошедшего времени в каждой из частей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c243166-1d92-4593-8594-a5e0219a29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bible_gram_rel_counter(data, grammeme):\n",
    "    counters_gramm = []\n",
    "    for book in tqdm(data):\n",
    "        num_verbs = len(book)\n",
    "        counter = 0\n",
    "        for word in book:\n",
    "            if word[1] == str(grammeme):\n",
    "                counter += 1\n",
    "            else: \n",
    "                continue\n",
    "        counters_gramm.append(counter / num_verbs * 100)\n",
    "    return (counters_gramm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e1a08-5f20-4f95-a338-c2ca819466a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_verbs = bible_gram_rel_counter(all_books, 'past')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb96c2f-e714-44df-a558-393faed5cc9a",
   "metadata": {},
   "source": [
    "Осталось нанести на график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a570a-5dd8-4bc8-91c2-d7435a05ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(past_verbs))\n",
    "book_names = ['Тора', 'Книга Иисуса Навина', 'Новый Завет']\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x=x, y=past_verbs, marker='o', label='Прошедшее время')\n",
    "plt.xlabel(\"Часть Библии\")\n",
    "plt.ylabel(\"Доля глагола\")\n",
    "plt.title(\"Частота глаголов прошедшего времени\")\n",
    "plt.xticks(x, book_names)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f19ac-ebb8-43e1-ac13-dd9bf12ba029",
   "metadata": {},
   "source": [
    "**Задание 4 (для самых отчаянных).** Переделайте код так, что он оставил только существительные и их падежи. Сделайте два графика:\n",
    "1. На первом только один плот с несколькими линями, где каждая линия - конкретный падеж;\n",
    "2. Сделайте плитку из плотов, где на каждом плоте одна линия конкретного падежа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b777a6c6-3328-46a8-aab6-2d48f47418fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
